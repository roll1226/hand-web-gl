<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>手を動かしてグラデションを動かすやーつ</title>
    <script src="https://unpkg.com/three@0.139.2/build/three.min.js"></script>

    <!-- 3つのライブラリを読み込む -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
  </head>
  <body>
    <div class="modal" data-show="true">
      <div class="modal__card">
        <button class="modal__card__btn modal__card__btn--normal">
          モーションキャプチャーなし
        </button>

        <button class="modal__card__btn modal__card__btn--hand">
          モーションキャプチャーあり
        </button>
        <span> ※「モーションキャプチャーあり」の場合、カメラが起動します </span>
      </div>
    </div>

    <canvas id="webgl-canvas"></canvas>
    <div class="container">
      <!-- Webカメラの映像（入力） -->
      <video id="input"></video>
      <!--  認識した手の形状を可視化した映像（出力）  -->
      <canvas id="output" width="300" height="200"></canvas>
    </div>

    <!-- <button id="start">start</button>
    <button id="stop">stop</button> -->

    <script>
      const modal = document.querySelector(".modal");
      let isHand = false;
      document
        .querySelector(".modal__card__btn--normal")
        .addEventListener("click", () => {
          modal.dataset.show = "false";
        });

      document
        .querySelector(".modal__card__btn--hand")
        .addEventListener("click", () => {
          modal.dataset.show = "false";
          camera.start();
          isHand = true;
        });

      const video = document.getElementById("input");
      const canvas = document.getElementById("output");
      const ctx = canvas.getContext("2d");

      const config = {
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`,
      };
      const hands = new Hands(config);

      //カメラからの映像をhands.jsで使えるようにする
      const camera = new Camera(video, {
        onFrame: async () => {
          await hands.send({ image: video });
        },
        width: 600,
        height: 400,
      });

      hands.setOptions({
        maxNumHands: 2,
        modelComplexity: 1,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });

      let codeX = 0;
      let codeY = 0;

      //形状認識した結果の取得
      hands.onResults((results) => {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);

        if (results.multiHandLandmarks) {
          results.multiHandLandmarks.forEach((marks) => {
            // 緑色の線で骨組みを可視化
            drawConnectors(ctx, marks, HAND_CONNECTIONS, { color: "#0f0" });

            // 赤色でランドマークを可視化
            drawLandmarks(ctx, marks, { color: "#f00" });
            console.log(marks);
            marks.forEach((mark) => {
              codeX += (mark.x - mark.y) % mark.z;
            });
          });
        }
      });
    </script>

    <!-- vertexShader -->
    <script id="js-vertex-shader" type="x-shader/x-vertex">
      attribute vec3 position;
      attribute vec2 uv;
      varying vec2 vUv;

      void main()	{
        vUv = uv;
        gl_Position = vec4(position, 1.0);
      }
    </script>

    <!-- fragmentShader -->
    <script id="js-fragment-shader" type="x-shader/x-fragment">
      precision mediump float;
      uniform float time;
      varying vec2 vUv;
      void main() {
        vec2 p = vUv * 1.0 - 0.5;
        float r = 1.0 + 0.5 * (sin(5.0 * p.x + time));
        float g = 1.0 + 0.5 * (sin(5.0 * p.y) + sin(time + 2.0 * p.x));
        float b = 1.0 + 0.5 * (sin(5.0 + p.x * p.y * 17.0) + sin(time * 0.4  + 4.0 * p.y));
        gl_FragColor = vec4(r, g, b, 1.0);
      }
    </script>

    <script>
      class Stage {
        constructor() {
          this.renderParam = {
            width: window.innerWidth,
            height: window.innerHeight,
          };

          this.cameraParam = {
            left: -1,
            right: 1,
            top: 1,
            bottom: -1,
            near: 0,
            far: -1,
          };

          this.scene = null;
          this.camera = null;
          this.renderer = null;
          this.geometry = null;
          this.material = null;
          this.mesh = null;

          this.isInitialized = false;
        }

        init() {
          this._setScene();
          this._setRender();
          this._setCamera();

          this.isInitialized = true;
        }

        _setScene() {
          this.scene = new THREE.Scene();
        }

        _setRender() {
          this.renderer = new THREE.WebGLRenderer({
            canvas: document.getElementById("webgl-canvas"),
          });
          this.renderer.setSize(
            this.renderParam.width,
            this.renderParam.height
          );
        }

        _setCamera() {
          if (!this.isInitialized) {
            this.camera = new THREE.OrthographicCamera(
              this.cameraParam.left,
              this.cameraParam.right,
              this.cameraParam.top,
              this.cameraParam.bottom,
              this.cameraParam.near,
              this.cameraParam.far
            );
          }

          const windowWidth = window.innerWidth;
          const windowHeight = window.innerHeight;

          this.camera.aspect = windowWidth / windowHeight;

          this.camera.updateProjectionMatrix();
          this.renderer.setSize(windowWidth, windowHeight);
        }

        _render() {
          this.renderer.render(this.scene, this.camera);
        }

        onResize() {
          this._setCamera();
        }

        onRaf() {
          this._render();
        }
      }

      class Mesh {
        constructor(stage) {
          this.geometryParm = {};

          this.materialParam = {
            useWireframe: false,
          };

          this.uniforms = {
            time: { type: "f", value: 1.0 },
          };

          this.stage = stage;

          this.mesh = null;

          this.windowWidth = 0;
          this.windowHeight = 0;

          this.windowWidthHalf = 0;
          this.windowHeightHalf = 0;
        }

        init() {
          this._setMesh();
        }

        _setMesh() {
          const geometry = new THREE.PlaneBufferGeometry(2, 2);
          const material = new THREE.RawShaderMaterial({
            vertexShader:
              document.getElementById("js-vertex-shader").textContent,
            fragmentShader:
              document.getElementById("js-fragment-shader").textContent,
            uniforms: this.uniforms,
          });

          this.mesh = new THREE.Mesh(geometry, material);

          this.stage.scene.add(this.mesh);
        }

        _render() {
          if (isHand) {
            this.uniforms.time.value = codeX;
          } else {
            this.uniforms.time.value += 0.05;
          }
        }

        onRaf() {
          this._render();
        }
      }

      (() => {
        const stage = new Stage();

        stage.init();

        const mesh = new Mesh(stage);

        mesh.init();

        window.addEventListener("resize", () => {
          stage.onResize();
        });

        const _raf = () => {
          window.requestAnimationFrame(() => {
            stage.onRaf();
            mesh.onRaf();

            _raf();
          });
        };

        _raf();
      })();
    </script>
  </body>
</html>
